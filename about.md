---
layout: page
title: About
---

<!-- <p class="message">
  Hey there! This page is included as an example. Feel free to customize it for your own use upon downloading. Carry on!
</p> -->

Hello! I'm Krishna Agaram, a final-year CS undergrad at IIT Bombay (update: I just graduated, woohoo! Heading to grad school at UIUC). I enjoy robots (in particular, controlling underactuated systems), deep learning theory, pure math and cool dev projects. I've worked on reinforcement learning, property testing, probabilistic proofs and quantum information.

If I'm not working some math, I'm likely reading Asimov or Christie, playing the piano, or am outdoors on a run or bike, climb or hike. I'm always looking to learn new things and meet new people, so feel free to connect if any of the above topics interest you too!

My (old, Autumn 2024) [CV](https://mathismusic.github.io/krishna-agaram-latest.pdf), [Github](https://github.com/mathismusic) and [LinkedIn](https://www.linkedin.com/in/krishna-n-agaram-5a4753324/).

<!-- P.S. this website is a stub and is mostly a placeholder for now. A few old blog entries on the home page, that's all. More to come soon! -->

I sometimes make notes, which you can find [here](https://github.com/mathismusic/notes). I am a big supporter of [inquiry-based learning](https://en.wikipedia.org/wiki/Inquiry-based_learning); if you're interested, I encourage checking out [JIBLM](https://jiblm.org/guides/index.php?category=jiblmjournal). I have authored one such guide to [Linear Algebra](https://mathismusic.github.io/ibl-linear-alg.pdf) and a more elementary, story-book guide to [Counting](https://mathismusic.github.io/story-draft.pdf) (draft). Finally, I have a [dead blog](https://mathismusic.github.io/website2024) that needs revival.

### Coursework

<!-- I was previously very theory-oriented, coming from a math background, and my coursework sort of reflects a bias in that direction. However, over the last year, I've realized that I get much more satisfaction when other people can benefit from what I do, and so I'm moving towards more applied stuff. Of course, math et al is still fun, and will remain a top hobby. -->
I like theory! --- in recent times, when it can guide practice.

- **undergraduate CS theory**: data structures, algorithm analysis, logic, automata theory, cryptography, game theory and mechanism design
- **graduate CS theory**: quantum information, spectral graph theory, approximation algorithms, randomized algorithms, probabilistic proofs\*, theoretical machine learning, statistical learning theory\*, theory of deep learning\*, formal methods in machine learning
- **math**: linear algebra, differential equations, calculus 1&2, analysis, abstract algebra, probability and statistics, numerical analysis, galois theory\*, fourier analysis\*, extremal graph theory and graph regularity (graduate)
- **undergraduate systems**: software systems, architecture, networks, operating systems, databases, compilers, advanced compilers\*
<!-- - **other** -->

(* -- ongoing)
<!-- Additionally, I read up things that take my fancy: so far, complex analysis, analytic combinatorics, quantum algorithms and error correction, probabilistic proofs -->



### Misc

Some  remarks (which certainly shouldn't be on the main page, I apologize)

- One of my all-time favorite topics is [Analytic Combinatorics](https://ac.cs.princeton.edu/home/AC.pdf), an area that marries combinatorics to algebra and complex analysis. It's a beautiful book, built upon the composition of seemingly elementary operations applied to classes of combinatorial objects to build new ones, and I highly recommend it to anyone interested in generating functions. My dream is to one day have an opportunity to use it in CS theory research.
- Recently, after a course on theoretical machine learning, I have been giving some vague thought to a complexity-theoretic analysis of machine learning approaches: is it possible to have a complexity theory of deep learning, with problems falling into different (parameterized by size, surely) complexity classes corresponding to (say) different architectures? Essentially, if we think of machine learning as a new paradigm of computing, then this is simply its complexity theory. I'm not sure if this is a well-defined question, but it's interesting to think about.
